{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import numpy as np\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unique(arr, dic=None):\n",
    "    if (dic is None):\n",
    "        dic = {}\n",
    "    for el in arr:\n",
    "        if isinstance(el, list):\n",
    "            unique(el, dic)\n",
    "        else:\n",
    "            if (el not in dic):\n",
    "                dic[el] = 1\n",
    "            else:\n",
    "                dic[el] += 1\n",
    "    return np.array(dic.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классификация будет происходить по след формуле:\n",
    "$$p(c\\mid d,\\lambda)=\\frac\n",
    "{\\exp\\sum_i^{n \\times k}{\\lambda_i}f_i\\left(c,d\\right )}\n",
    "{\\sum_{\\tilde{c}\\in C}{\\exp\\sum_i^{n \\times k}{\\lambda_i}f_i\\left(\\tilde{c},d\\right )}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(x, weights, y_patterns):\n",
    "    # начальное приведение\n",
    "    probas = np.ones(weights.shape[1]) * np.log(1.0 / weights.shape[1])\n",
    "\n",
    "    # считаем условные вероятности\n",
    "    for xi in x:\n",
    "        v =  weights[xi] * y_patterns[xi]\n",
    "        probas += v\n",
    "\n",
    "    # далее сглаживаем выходы через softmax\n",
    "    probas = np.exp(probas / weights.shape[1])\n",
    "    return probas / np.sum(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задачу будем решать с помощью максимизации функции правдоподобия\n",
    "$$\\log p(C|D,\\lambda)\n",
    "=\\sum_{(c,d)\\in(C,D)}\\log p(c|d,\\lambda)\n",
    "=\\sum_{(c,d)\\in(C,D)}\\log\\frac\n",
    "{\\exp\\sum_i^{n \\times k}{\\lambda_i}f_i\\left(c,d\\right )}\n",
    "{\\sum_{\\tilde{c}\\in C}{\\exp\\sum_i^{n \\times k}{\\lambda_i}f_i\\left(\\tilde{c},d\\right )}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соответственно градиент у нас будет в частных производных\n",
    "\n",
    "$$\\frac{\\partial\\log p(C|D,\\lambda)}{\\partial\\lambda_i}=\n",
    "\\sum_{(c,d)\\in(C,D)}{f_i(c,d)}-\n",
    "\\sum_{d\\in D}{\\sum_{c\\in C}{p(c|d,\\lambda)f_i(c,d)}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit(X, y, f_count, c_count, alpha=0.85, max_iter=100, tol=0.00001, random_state=None, verbose=1):\n",
    "    n_samples = len(X)\n",
    "    if random_state is not None:\n",
    "        random.seed(random_state)\n",
    "\n",
    "#     # определяем сколько у нас уникальных токенов\n",
    "#     features = unique(X)\n",
    "#     f_count = features.shape[0]\n",
    "#     # определяем сколько у нас уникальных классов\n",
    "#     classes = unique(y)\n",
    "#     c_count = classes.shape[0]\n",
    "    \n",
    "    # матрица индикаторов(условных признаков)\n",
    "    feature_patterns = np.zeros((f_count, c_count), dtype=np.int)\n",
    "\n",
    "    # матрица весов индикаторов\n",
    "    weights = np.zeros((f_count, c_count))\n",
    "\n",
    "    # инициализация индикаторов\n",
    "    for i in range(n_samples):\n",
    "        for xi in X[i]:\n",
    "            feature_patterns[xi, y[i]] = 1\n",
    "\n",
    "    #\n",
    "    prev_logl = 0.\n",
    "    iter_num = 0\n",
    "    all_iter = 0\n",
    "    # ограничим сверху max_iter итерациями\n",
    "    for iter_num in range(max_iter):\n",
    "        if verbose:\n",
    "            print 'Start iteration #%d\\t' % iter_num,\n",
    "\n",
    "        logl = 0.\n",
    "        ncorrect = 0\n",
    "\n",
    "        # random прохождение существенно улучшает схождение SGD\n",
    "        r = range(n_samples)\n",
    "        r = random.sample(r, n_samples)\n",
    "        iter_sample = 0\n",
    "        for i in r:\n",
    "            iter_sample += 1\n",
    "\n",
    "            if verbose and iter_sample % (n_samples / 20) == 0:\n",
    "                print '.',\n",
    "\n",
    "            all_iter += 1\n",
    "            eta = alpha ** (all_iter / n_samples)\n",
    "            # предсказываем вероятности\n",
    "            probas = predict(X[i], weights, feature_patterns)\n",
    "\n",
    "            # смотрим, правильно ли мы предсказали, это нужно только для verbose\n",
    "            if np.argmax(probas) == y[i]:\n",
    "                ncorrect += 1\n",
    "            # считаем \"правдоподобие\"\n",
    "            logl += np.log(probas[y[i]]) / features.shape[0]\n",
    "\n",
    "            # обновляем веса\n",
    "            for j in range(len(X[i])):\n",
    "                conditional_y = feature_patterns[X[i][j]]\n",
    "                for y_i in range(len(conditional_y)):\n",
    "                    # ожидание\n",
    "                    expected_ent = 1.0 if conditional_y[y_i] == 1 and y_i == y[i] else 0.0\n",
    "                    # реальность\n",
    "                    max_ent = probas[y_i]\n",
    "                    weights[X[i][j], y_i] -= (max_ent - expected_ent) * eta  #\n",
    "        if verbose:\n",
    "            print '\\tAccuracy: %.5f, Loss: %.8f' % (1.0 * ncorrect / n_samples, logl - prev_logl)\n",
    "        if iter_num > 0:\n",
    "            if prev_logl > logl:\n",
    "                print('there is model diverging')\n",
    "                break\n",
    "            if (logl - prev_logl) < tol:\n",
    "                break\n",
    "        prev_logl = logl\n",
    "    print iter_num\n",
    "    return weights, feature_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "digits_regex = re.compile('\\d')\n",
    "punc_regex = re.compile('[\\%\\(\\)\\-\\/\\:\\;\\<\\>\\«\\»\\,]')\n",
    "delim_regex = re.compile('([\\.])\\s+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_and_tokenize(foldername):\n",
    "    '''\n",
    "    метод для считывания текстов из файлов папки\n",
    "    здесь применяется довольно простая токенизация\n",
    "    '''\n",
    "    word2index = {}\n",
    "    word_counts = {}\n",
    "    \n",
    "    index2word = []\n",
    "    i = 0\n",
    "    tokenized_text = []\n",
    "    for path, subdirs, files in os.walk('data'):\n",
    "        for name in files:\n",
    "            filename = os.path.join(path, name)\n",
    "            with io.open(filename, 'r', encoding='utf-8') as data_file:\n",
    "                for line in data_file:\n",
    "                    if len(line) < 50:\n",
    "                        continue\n",
    "                    text = digits_regex.sub(u'0', line.lower())\n",
    "                    text = punc_regex.sub(u'', text)\n",
    "                    text = delim_regex.sub(r' \\1 ', text)\n",
    "                    for word in text.split():\n",
    "                        if word and word not in word2index:\n",
    "                            word2index[word] = i\n",
    "                            index2word.append(word)\n",
    "                            i += 1\n",
    "                        tokenized_text.append(word)\n",
    "    return tokenized_text, word2index, index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_train(tokenized_text, word2index,context_len = 4):\n",
    "    ''' \n",
    "    метод для генерации обучающих данных\n",
    "    '''\n",
    "    X = []\n",
    "    y = []\n",
    "    for i, y_word in enumerate(tokenized_text):\n",
    "        x = []\n",
    "        for j in range(i - context_len, i):\n",
    "            if (j >= 0):\n",
    "                x_word = tokenized_text[j]\n",
    "                x.append(word2index[x_word])\n",
    "        if (len(x) > 0):\n",
    "            X.append(x)\n",
    "            y.append(word2index[y_word])\n",
    "        if(i % 10000 == 0):\n",
    "            print 'i =', i\n",
    "    print 'end'\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized_text, word2index, index2word = read_and_tokenize('data')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all words: 287632\n",
      "all unique words 43030\n"
     ]
    }
   ],
   "source": [
    "unique_words = len(index2word)\n",
    "print 'all words:', len(tokenized_text)\n",
    "print 'all unique words', unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i = 0\n",
      "i = 10000\n",
      "i = 20000\n",
      "i = 30000\n",
      "i = 40000\n",
      "i = 50000\n",
      "i = 60000\n",
      "i = 70000\n",
      "i = 80000\n",
      "i = 90000\n",
      "i = 100000\n",
      "i = 110000\n",
      "i = 120000\n",
      "i = 130000\n",
      "i = 140000\n",
      "i = 150000\n",
      "i = 160000\n",
      "i = 170000\n",
      "i = 180000\n",
      "i = 190000\n",
      "i = 200000\n",
      "i = 210000\n",
      "i = 220000\n",
      "i = 230000\n",
      "i = 240000\n",
      "i = 250000\n",
      "i = 260000\n",
      "i = 270000\n",
      "i = 280000\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "context_len = 4\n",
    "X,y = generate_train(tokenized_text, word2index,context_len=context_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-3db6968a8a37>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpatterns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0munique_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0munique_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m241\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-17a60f9ff690>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(X, y, f_count, c_count, alpha, max_iter, tol, random_state, verbose)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m# матрица весов индикаторов\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m# инициализация индикаторов\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "weights, patterns = fit(X, y,unique_words,unique_words,random_state=241)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = [word2index[u'экономическая'],word2index[u'ситуация']]\n",
    "for i in range(10):\n",
    "    pred = predict(test, weights, patterns)\n",
    "    index = np.argmax(pred)\n",
    "    print index2word[index],\n",
    "    test.append(index)\n",
    "    if len(test) > context_len:\n",
    "        del test[0]\n",
    "    print test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
