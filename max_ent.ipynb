{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import numpy as np\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unique(arr, dic=None):\n",
    "    if (dic is None):\n",
    "        dic = {}\n",
    "    for el in arr:\n",
    "        if isinstance(el, list):\n",
    "            unique(el, dic)\n",
    "        else:\n",
    "            if (el not in dic):\n",
    "                dic[el] = 1\n",
    "            else:\n",
    "                dic[el] += 1\n",
    "    return np.array(dic.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классификация будет происходить по след формуле:\n",
    "$$p(c\\mid d,\\lambda)=\\frac\n",
    "{\\exp\\sum_i^{n \\times k}{\\lambda_i}f_i\\left(c,d\\right )}\n",
    "{\\sum_{\\tilde{c}\\in C}{\\exp\\sum_i^{n \\times k}{\\lambda_i}f_i\\left(\\tilde{c},d\\right )}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(x, weights, feature_patterns, n_classes):\n",
    "    # начальное приведение\n",
    "    probas = np.ones(n_classes) * np.log(1.0 / n_classes)\n",
    "\n",
    "    # считаем условные вероятности\n",
    "    for xi in x:\n",
    "        for i in xrange(len(feature_patterns[xi])):\n",
    "            probas[feature_patterns[xi][i]] += weights[xi][i]\n",
    "\n",
    "    # далее сглаживаем выходы через softmax\n",
    "    probas = np.exp(probas / n_classes)\n",
    "    return probas / np.sum(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задачу будем решать с помощью максимизации функции правдоподобия\n",
    "$$\\log p(C|D,\\lambda)\n",
    "=\\sum_{(c,d)\\in(C,D)}\\log p(c|d,\\lambda)\n",
    "=\\sum_{(c,d)\\in(C,D)}\\log\\frac\n",
    "{\\exp\\sum_i^{n \\times k}{\\lambda_i}f_i\\left(c,d\\right )}\n",
    "{\\sum_{\\tilde{c}\\in C}{\\exp\\sum_i^{n \\times k}{\\lambda_i}f_i\\left(\\tilde{c},d\\right )}}$$\n",
    "\n",
    "Соответственно градиент у нас будет в частных производных\n",
    "\n",
    "$$\\frac{\\partial\\log p(C|D,\\lambda)}{\\partial\\lambda_i}=\n",
    "\\sum_{(c,d)\\in(C,D)}{f_i(c,d)}-\n",
    "\\sum_{d\\in D}{\\sum_{c\\in C}{p(c|d,\\lambda)f_i(c,d)}}$$\n",
    "\n",
    "итого:\n",
    "$$w^{k+1} = w^{k} + \\eta_k\\frac{\\partial}{\\partial w_i}(L(j,w) - \\frac{C}{N}|w_i|)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# L(j,w)\n",
    "def apply_l1_penalty(i, j, u, weights, q):\n",
    "    z = weights[i][j]\n",
    "    if weights[i][j] > 0:\n",
    "        weights[i][j] =  max(0.0, weights[i][j] - (u + q[i][j]))\n",
    "    elif weights[i][j] < 0:\n",
    "        weights[i][j] =  max(0.0, weights[i][j] + (u - q[i][j]))\n",
    "    q[i][j] = weights[i][j] - z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fit(X, y, f_count, c_count, batch_size=64, alpha=0.85, max_iter=100, tol=0.00001, l1param=0.1, random_state=None, verbose=1):\n",
    "    '''\n",
    "        X - контекст \n",
    "        y - цель\n",
    "        f_count - количество уникальных признаков в контексте\n",
    "        c_count - количество уникальных целей, по факту искомые классы\n",
    "        batch_size - размер подвыборки для обучения\n",
    "        alpha - быстрота обучения\n",
    "        max_iter - макс кол-во итераций\n",
    "        tol - порог смещения градиента\n",
    "        l1param - параметр l1 регуляризации\n",
    "        random_state - для репродуцирования эксперимента\n",
    "        verbose - дебаг\n",
    "    '''\n",
    "    n_samples = len(X)\n",
    "    if batch_size is None:\n",
    "        batch_size = n_samples\n",
    "    if batch_size > n_samples:\n",
    "        batch_size = n_samples\n",
    "    if random_state is not None:\n",
    "        random.seed(random_state)\n",
    "\n",
    "#     # определяем сколько у нас уникальных токенов\n",
    "#     features = unique(X)\n",
    "#     f_count = features.shape[0]\n",
    "#     # определяем сколько у нас уникальных классов\n",
    "#     classes = unique(y)\n",
    "#     c_count = classes.shape[0]\n",
    "\n",
    "    # матрица индикаторов(условных признаков)\n",
    "    feature_patterns = [[] for _ in range(f_count)]\n",
    "    f_pattern_set = {}\n",
    "    # матрица весов индикаторов\n",
    "    weights = [[] for _ in range(f_count)]\n",
    "    q =  [[] for _ in range(f_count)]\n",
    "    # инициализация индикаторов\n",
    "    for i in range(n_samples):\n",
    "        for xi in X[i]:\n",
    "            if xi not in f_pattern_set:\n",
    "                f_pattern_set[xi] = set()\n",
    "            if y[i] not in f_pattern_set[xi]:\n",
    "                f_pattern_set[xi].add(y[i])\n",
    "                weights[xi].append(0.0)\n",
    "                q[xi].append(0.0)\n",
    "                feature_patterns[xi].append(y[i])\n",
    "    prev_logl = 0.\n",
    "    iter_num = 0\n",
    "    all_iter = 0\n",
    "    u = 0.0\n",
    "    # ограничим сверху max_iter итерациями\n",
    "    for iter_num in range(max_iter):\n",
    "        if verbose:\n",
    "            print 'Start iteration #%d\\t' % iter_num,\n",
    "\n",
    "        logl = 0.\n",
    "        ncorrect = 0\n",
    "\n",
    "        # random прохождение существенно улучшает схождение SGD\n",
    "        r = range(n_samples)\n",
    "        r = random.sample(r, batch_size)\n",
    "        iter_sample = 0\n",
    "        for i in r:\n",
    "            iter_sample += 1\n",
    "            if verbose and (batch_size < 20 or iter_sample % (batch_size / 20) == 0):\n",
    "                print '.',\n",
    "\n",
    "            all_iter += 1\n",
    "            eta = alpha ** (all_iter / n_samples)\n",
    "            # предсказываем вероятности\n",
    "            probas = predict(X[i], weights, feature_patterns, c_count)\n",
    "\n",
    "            # смотрим, правильно ли мы предсказали, это нужно только для verbose\n",
    "            if np.argmax(probas) == y[i]:\n",
    "                ncorrect += 1\n",
    "            # считаем \"правдоподобие\"\n",
    "            logl += np.log(probas[y[i]])\n",
    "            \n",
    "            u += eta * l1param;\n",
    "            # обновляем веса\n",
    "            for j in range(len(X[i])):\n",
    "                conditional_y = feature_patterns[X[i][j]]\n",
    "                for y_i in xrange(len(conditional_y)):\n",
    "                    # ожидание\n",
    "                    expected_ent = 1.0 if conditional_y[y_i] == y[i] else 0.0\n",
    "                    # реальность\n",
    "                    max_ent = probas[conditional_y[y_i]]\n",
    "                    weights[X[i][j]][y_i] -= 1.0 * (max_ent - expected_ent) * eta\n",
    "                    apply_l1_penalty(X[i][j],y_i,u,weights,q)\n",
    "        logl /= c_count\n",
    "        if verbose:\n",
    "            f = logl\n",
    "            sum = 0.0\n",
    "            for val_x in weights:\n",
    "                for val_y in val_x:\n",
    "                    sum += abs(val_y)\n",
    "            f -= l1param * sum;\n",
    "            print '\\tAccuracy: %.5f, Loss: %.8f' % (1.0 * ncorrect / batch_size, f)\n",
    "        if iter_num > 0:\n",
    "            if prev_logl > logl:\n",
    "                break\n",
    "            if (logl - prev_logl) < tol:\n",
    "                break\n",
    "        prev_logl = logl\n",
    "    print iter_num\n",
    "    return weights, feature_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start iteration #0\t. . . . . . . . . . \tAccuracy: 0.70000, Loss: -3.08720890\n",
      "Start iteration #1\t. . . . . . . . . . \tAccuracy: 0.90000, Loss: -2.15663061\n",
      "Start iteration #2\t. . . . . . . . . . \tAccuracy: 0.80000, Loss: -1.81543056\n",
      "Start iteration #3\t. . . . . . . . . . \tAccuracy: 0.80000, Loss: -1.48595419\n",
      "Start iteration #4\t. . . . . . . . . . \tAccuracy: 0.90000, Loss: -1.39364816\n",
      "Start iteration #5\t. . . . . . . . . . \tAccuracy: 0.90000, Loss: -1.27724560\n",
      "Start iteration #6\t. . . . . . . . . . \tAccuracy: 0.80000, Loss: -1.28469326\n",
      "6\n",
      "[ 0.9355966  0.0644034]\n"
     ]
    }
   ],
   "source": [
    "# небольшой тест\n",
    "X = [[0, 1],\n",
    "     [2, 1],\n",
    "     [2, 3],\n",
    "     [2, 1],\n",
    "     [0, 1],\n",
    "     [2, 1, 4],\n",
    "     [2, 3, 4],\n",
    "     [2, 1, 5],\n",
    "     [0, 3, 5],\n",
    "     [0, 1, 5]]\n",
    "\n",
    "y = [0, 0, 1, 1, 0, 1, 1, 0, 0, 0]\n",
    "# определяем сколько у нас уникальных токенов\n",
    "features = unique(X)\n",
    "f_count = features.shape[0]\n",
    "# определяем сколько у нас уникальных классов\n",
    "classes = unique(y)\n",
    "c_count = classes.shape[0]\n",
    "weights, patterns = fit(X, y,f_count,c_count, random_state=241,l1param=0.00001)\n",
    "# print weights\n",
    "# print patterns\n",
    "\n",
    "pred = predict([0, 1], weights, patterns,c_count)\n",
    "print pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "digits_regex = re.compile('\\d')\n",
    "punc_regex = re.compile('[\\%\\(\\)\\-\\/\\:\\;\\<\\>\\«\\»\\,]')\n",
    "delim_regex = re.compile('([\\.])\\s+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_and_tokenize(foldername):\n",
    "    '''\n",
    "    метод для считывания текстов из файлов папки\n",
    "    здесь применяется довольно простая токенизация\n",
    "    '''\n",
    "\n",
    "    word_counts = {}\n",
    "    tokenized_text = []\n",
    "    for path, subdirs, files in os.walk('data'):\n",
    "        for name in files:\n",
    "            filename = os.path.join(path, name)\n",
    "            with io.open(filename, 'r', encoding='utf-8') as data_file:\n",
    "                for line in data_file:\n",
    "                    if len(line) < 50:\n",
    "                        continue\n",
    "                    text = digits_regex.sub(u'0', line.lower())\n",
    "                    text = punc_regex.sub(u'', text)\n",
    "                    text = delim_regex.sub(r' \\1 ', text)\n",
    "                    for word in text.split():\n",
    "                        if not word:\n",
    "                            continue\n",
    "                        if word not in word_counts:\n",
    "                            word_counts[word] = 1\n",
    "                        else:\n",
    "                            word_counts[word] += 1\n",
    "                        tokenized_text.append(word)\n",
    "    word2index = {}\n",
    "    index2word = []\n",
    "    i = 0\n",
    "    filtered_text = []\n",
    "    for word in tokenized_text:\n",
    "        if word_counts[word] > 2:\n",
    "            if word not in word2index:\n",
    "                word2index[word] = i\n",
    "                index2word.append(word)\n",
    "                i += 1\n",
    "            filtered_text.append(word)\n",
    "\n",
    "\n",
    "    return filtered_text, word2index, index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_train(tokenized_text, word2index,context_len = 4):\n",
    "    '''\n",
    "    метод для генерации обучающих данных\n",
    "    '''\n",
    "    X = []\n",
    "    y = []\n",
    "    for i, y_word in enumerate(tokenized_text):\n",
    "        x = []\n",
    "        for j in range(i - context_len, i):\n",
    "            if (j >= 0):\n",
    "                x_word = tokenized_text[j]\n",
    "                x.append(word2index[x_word])\n",
    "        if (len(x) > 0):\n",
    "            X.append(x)\n",
    "            y.append(word2index[y_word])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized_text, word2index, index2word = read_and_tokenize('data')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all words: 45825\n",
      "all unique words: 3872\n"
     ]
    }
   ],
   "source": [
    "unique_words = len(index2word)\n",
    "print 'all words:', len(tokenized_text)\n",
    "print 'all unique words:', unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "context_len = 4\n",
    "X,y = generate_train(tokenized_text, word2index,context_len=context_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start iteration #0\t. . . . . . . . . . . . . . . . . . . . \tAccuracy: 0.04395, Loss: -5.60062674\n",
      "Start iteration #1\t. . . . . . . . . . . . . . . . . . . . \tAccuracy: 0.04883, Loss: -12.27356474\n",
      "Start iteration #2\t. . . . . . . . . . . . . . . . . . . . \tAccuracy: 0.05469, Loss: -28.91910292\n",
      "Start iteration #3\t. . . . . . . . . . . . . . . . . . . . \tAccuracy: 0.07520, Loss: -50.19205419\n",
      "Start iteration #4\t. . . . . . . . . . . . . . . . . . . . \tAccuracy: 0.08691, Loss: -100.62756221\n",
      "Start iteration #5\t. . . . . . . . . . . . . . . . . . . . \tAccuracy: 0.10156, Loss: -114.63170959\n",
      "Start iteration #6\t. . . . . . . . . . . . . . . . . . . . \tAccuracy: 0.11230, Loss: -158.07853935\n",
      "Start iteration #7\t. . . . . . . . . . . . . . . . . . . . \tAccuracy: 0.09766, Loss: -78.07157679\n",
      "Start iteration #8\t. . . . . . . . . . . . . . . . . . . . \tAccuracy: 0.10547, Loss: -112.62013875\n",
      "Start iteration #9\t. . . . . . . . . . . . . . . . . . . . \tAccuracy: 0.10840, Loss: -363.42530582\n",
      "Start iteration #10\t. . . . . . . . . . . . . . . . . . . . \tAccuracy: 0.09473, Loss: -198.79079080\n",
      "Start iteration #11\t. . . . . . . . . . . . . . . . . . . . \tAccuracy: 0.08594, Loss: -288.66930158\n",
      "Start iteration #12\t. . . . . . . . . . . . . . . . . . . . \tAccuracy: 0.09277, Loss: -689.23042622\n",
      "Start iteration #13\t. . . . . . . . . . . . . . . . . . . . \tAccuracy: 0.09180, Loss: -345.37997632\n",
      "Start iteration #14\t. . . . . . . . . . . . . . . . . . . . \tAccuracy: 0.07031, Loss: -724.37842190\n",
      "Start iteration #15\t. . . . . . . . . . . . . . . . . . . . \tAccuracy: 0.08594, Loss: -749.12990765\n",
      "Start iteration #16\t. . . . . . . . . . . . . . . . . . . . \tAccuracy: 0.08203, Loss: -265.23484483\n",
      "Start iteration #17\t. . . . . . . . . . . . . . . . . . . . \tAccuracy: 0.07324, Loss: -1314.11304759\n",
      "Start iteration #18\t. . . . . . . . . . . . . . . . . . . . \tAccuracy: 0.10352, Loss: -411.14021859\n",
      "Start iteration #19\t. . . . . . . . . . . . . . . . . . . . \tAccuracy: 0.08008, Loss: -918.98172020\n",
      "Start iteration #20\t. . . . . . . . . . . . . . . . . . . . \tAccuracy: 0.08008, Loss: -1499.36444162\n",
      "Start iteration #21\t. . . . . . . . . . . . . . . . . . . . \tAccuracy: 0.08984, Loss: -1597.95628498\n",
      "Start iteration #22\t. . . . . . . . . . . . . . . . . . . . \tAccuracy: 0.07910, Loss: -969.23987828\n",
      "Start iteration #23\t. . . . . . . . . . . . . . . . . . . . \tAccuracy: 0.08984, Loss: -1003.61964165\n",
      "Start iteration #24\t. . . . . . . . . . . . . . . . . . . . \tAccuracy: 0.10254, Loss: -2485.09738552\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "weights, patterns = fit(X, y,unique_words,unique_words,random_state=241,verbose=1,batch_size=1024, l1param=0.0001,tol=0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GENRATED TEXT:\n",
      "украинским комиссии гагаузии помощи был и должен это решение по реформы 0000 в российский магнитского должен проверить стать о еще\n"
     ]
    }
   ],
   "source": [
    "test = [word2index[word] for word in [u'путин']]\n",
    "last_index = index = test[-1]\n",
    "print 'GENRATED TEXT:'\n",
    "for i in range(20):\n",
    "    pred = predict(test, weights, patterns,unique_words)\n",
    "    indicies = pred.argsort()[::-1][:20]\n",
    "    for index in indicies:\n",
    "        if index in test:\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    last_index = int(index)\n",
    "    print index2word[index],\n",
    "    test.append(index)\n",
    "    if len(test) > context_len:\n",
    "        del test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Идеи по улучшению\n",
    "* первое, что приходит на ум - это увеличить кол-во обучающей выборки\n",
    "* использовать в качестве контекста, не слова а символы с определнным окном(context_len) равным 40 или больше\n",
    "* использовать лематизацию или стемминг для словарных \"фич\", а затем скомбинировать с предыдущим пунктом(пока точно не представляю как)\n",
    "* модель работает немного медленно, а на больших текстах очень медленно. поэтому можно попробовать искать оптимальные параметры обучения. также можно переписать решение на С/С++ или на Сython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Использованная литература:\n",
    "* Tsuruoka Y., Tsujii J., Ananiadou S. Stochastic gradient descent training for l1-regularized log-linear models with cumulative penalty //Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 1-Volume 1. – Association for Computational Linguistics, 2009. – С. 477-485.\n",
    "* Smith N. A., Eisner J. Contrastive estimation: Training log-linear models on unlabeled data //Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics. – Association for Computational Linguistics, 2005. – С. 354-362.\n",
    "* Smith N. A. Log-Linear Models // Revised version of thesis research proposal, 2004"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
